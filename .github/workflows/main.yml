name: TLAiBench

on:
  workflow_dispatch:
    inputs:
      llm:
        description: 'LLM connection'
        required: false
        type: choice
        options:
          - '{"model": "github:openai/gpt-4.1", "provider": "github", "name": "gpt-4.1"}'
          - '{"model": "anthropic_bedrock:arn:aws:bedrock:us-east-1:024871859028:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0", "provider": "anthropic_bedrock", "name": "claude-sonnet-4"}'

## https://microsoft.github.io/genaiscript/reference/github-actions/
## https://docs.github.com/en/github-models/use-github-models/integrating-ai-models-into-your-development-workflow#using-ai-models-with-github-actions
permissions:
    models: read
    id-token: write   # required for OIDC
    contents: read

jobs:
  benchmarks:
    name: TLAiBench

    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash

    steps:

      - name: Clone repo
        uses: actions/checkout@v4
        with:
            ## All history for git diff below to succeed.
            fetch-depth: 0

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::024871859028:role/GitHubActionsRole
          aws-region: us-east-1

      ## Provision VSCode
      ##
      ## Note: VSCode is being used to host the TLA+ MCP servers. This choice is a a workaround, as VSCode is fundamentally a desktop/UI application,
      ## which introduces the complications below. I considered using code-server (https://github.com/coder/code-server), the headless version of VSCode,
      ## as an alternative. However, I couldn’t get it to automatically launch the TLA+ extension and its associated MCP servers. Another potential
      ## approach would be to refactor the TLA+ VSCode extension to allow the MCP servers to be launched independently—either as standalone processes or
      ## via GenAIScript.

      - name: Install VSCode
        run: |
          sudo apt-get update
          sudo apt-get install --no-install-recommends -y apt-transport-https
          curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg
          sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg
          echo "deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main" | sudo tee /etc/apt/sources.list.d/vscode.list
          sudo apt-get update
          sudo apt-get install --no-install-recommends -y code xvfb

      - name: Launch TLA+ VSCode extension hosting our MCP servers
        run: |
          ## Install TLA+ VSCode extension (doesn't require X11).
          code --install-extension tlaplus.vscode-ide
          ## Create X11 server to run VSCode.
          export XDG_RUNTIME_DIR=/run/user/$(id -u)
          export DBUS_SESSION_BUS_ADDRESS=unix:path=$XDG_RUNTIME_DIR/bus
          dbus-daemon --session --address=$DBUS_SESSION_BUS_ADDRESS --nofork --nopidfile --syslog-only &
          mkdir -p ~/.vscode && echo '{ "disable-hardware-acceleration": true }' > ~/.vscode/argv.json
          /usr/bin/Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
          ## Setting tlaplus.mcp.port and disabling workspace trust are the magic sauce that activates the TLA+ MCP Servers at startup.
          mkdir -p ~/.vscode && echo '{ "tlaplus.mcp.port": 59071 }' > .vscode/settings.json
          DISPLAY=:99 code --no-sandbox --disable-gpu --disable-workspace-trust .
          ## Allow vscode to come up before moving to ps and nc that validate that the MCP servers are up and running.
          #sleep 10
          #ps axu|grep vscode
          #nc -vz localhost 59071

      ## Provision TLA+ Tools
      ##

      - name: Get (nightly) TLC
        run: wget https://nightly.tlapl.us/dist/tla2tools.jar

      - name: Get (nightly) CommunityModules
        run: wget https://github.com/tlaplus/CommunityModules/releases/latest/download/CommunityModules-deps.jar

      ## Provision GenAIScript
      ##

      - name: Setup NodeJS
        ## https://github.com/actions/setup-node
        uses: actions/setup-node@v4
        with:
            node-version: "24"

      - name: Listing model configuration
        ## https://microsoft.github.io/genaiscript/reference/cli/#listing-model-configuration
        run: npx genaiscript scripts model genaisrc/translate.genai.mts
        env:
            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      ## Run TLAi benchmarks
      ##

      - name: Run GenAIscript on the logic puzzles.
        ## Install genaiscript runtime: https://microsoft.github.io/genaiscript/reference/cli/
        run: npx --yes genaiscript@latest run --provider $(echo '${{ github.event.inputs.llm }}' | jq -r .provider) --model $(echo '${{ github.event.inputs.llm }}' | jq -r .model) genaisrc/translate.genai.mts
        env:
            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      ## Postprocess the TLAi benchmarks
      ##

      - run: cat .genaiscript/runs/translate/*/trace.md
        if: always()

      - name: Normalize LLM name for artifact
        id: normalize
        run: |
          # Replace problematic characters with underscores
          normalized_llm=$(echo '${{ github.event.inputs.llm }}' | jq -r .name)
          echo "llm_name=${normalized_llm}" >> $GITHUB_OUTPUT

      - name: Archive generated TLA+ specs (if any)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          include-hidden-files: true
          name: TLAiBenchResults-${{ steps.normalize.outputs.llm_name }}
          path: |
            .genaiscript/*
            *.tla
            *.cfg
            !gold/*.tla
            !gold/*.cfg